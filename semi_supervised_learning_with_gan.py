# -*- coding: utf-8 -*-
"""Semi-Supervised Learning with Gan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K30k5lpm-3xlESOqirXU-e0AfW8_rvI8
"""

from numpy.random import randn, randint 
from numpy import expand_dims, zeros, ones, asarray 
from keras.datasets.mnist import load_data 
from tensorflow.keras.optimizers import Adam 
from keras.models import Model, Sequential 
from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose 
from keras.layers import LeakyReLU, Dropout, Lambda, Activation

import matplotlib.pyplot as plt
from keras import backend as K 
import numpy as np

#Define generator model 
def define_generator (latent_dim): 
  in_lat= Input (shape= (latent_dim,))
  #Reshape the latent dimension into 28*28*1 
  n_nodes= 256*7*7 
  x= Dense (n_nodes) (in_lat) 
  x= LeakyReLU (alpha=0.2) (x) 
  x= Reshape ((7,7,256))(x) 

  x= Conv2DTranspose (128, (3,3), strides= (2,2), padding='same')(x) 
  x= LeakyReLU (alpha=0.2)(x) 

  x= Conv2DTranspose (64, (3,3), strides= (1,1), padding='same')(x) 
  x= LeakyReLU (alpha=0.2)(x) 

  out_layer= Conv2DTranspose (1, (3,3), strides= (2,2), activation='tanh', padding='same')(x) 

  model= Model (in_lat, out_layer) 
  return model

#Descriminator

def define_discriminator (in_shape= (28,28,1), n_classes=10): 

  in_image= Input (shape=in_shape) 
  x= Conv2D (32, (3,3), strides= (2,2), padding='same') (in_image) 
  x= LeakyReLU (alpha=0.2) (x) 

  x= Conv2D (64, (3,3), strides= (2,2), padding='same') (x) 
  x= LeakyReLU (alpha=0.2) (x) 

  x= Conv2D (128, (3,3), strides= (2,2), padding='same') (x) 
  x= LeakyReLU (alpha=0.2) (x) 

  x= Flatten () (x) 
  x= Dropout (0.4) (x) 
  x= Dense (n_classes) (x) 

  model= Model (inputs= in_image, outputs= x)
  return model

# Supervised discriminator for multiclass classification

def supervised_descriminator (disc): 
  model= Sequential () 
  model.add(disc) 
  model.add (Activation ('softmax')) 
  model.compile (optimizer= Adam (lr=0.0002, beta_1=0.5),loss='sparse_categorical_crossentropy', metrics=['accuracy'] )
  return model

#Unsupervised discriminator model 
# Will have to take the output of supervised model, just before the activation function 
#Then add a layer with calculation of sum of exponential outputs 
#the custom activation layer gives a value close to 0 for smaller activation
#It gives value close to 1 for larger activation 
# This way it gives low activation for fake images and we dont need sigmoid anymore

# Custom activation function 
# D(x)= Z(x)/ (Z(X)+1) where, Z(x)= sum(exp(l(x))). l(x) is the output from supervise dis. prior to softmax

def custom_activation (x): 
  Z_x= K.sum (K.exp (x), axis=-1, keepdims=True) 
  D_x= Z_x / (Z_x+1)
  return D_x

def unsupervised_discriminator (disc): 
  model= Sequential () 
  model.add(disc) 
  model.add(Lambda (custom_activation)) 
  model.compile (loss='binary_crossentropy', optimizer= Adam (lr=0.0002, beta_1=0.5) )
  return model

# define combined generator-discriminator model 
def define_gan (gen_model, disc_unsupervised): 
  disc_unsupervised.trainable= False 
  gan_output= disc_unsupervised (gen_model.output) # the output of generator is the input to discriminator 
  model= Model (gen_model.input, gan_output) 
  model.compile (loss= 'binary_crossentropy', optimizer= Adam (lr=0.0002, beta_1= 0.5)) 
  return  model

# Load the images 
def load_real_samples (n_classes=10): 
  (train_x, train_y), (_,_)= load_data () 
  x= expand_dims (train_x, axis=-1) 
  x= x.astype ('float32') 
  x= (x-127.5)/127.5 
  return [x, train_y]

# Select subset of the dataset for supervised training 
# 10 samples per class total 100 samples

def select_supervised_sample (dataset, n_classes=10, n_samples=100): 
  x,y= dataset 
  x_list, y_list= list(), list () 
  n_per_class= int (n_samples / n_classes) 
  for i in range (n_classes): 
    x_with_class= x [y==i] #get all images of this class 
    ix= randint (0, len (x_with_class), n_per_class) 
    [x_list.append (x_with_class[j]) for j in ix] 
    [y_list.append (i) for j in ix]
    return asarray (x_list), asarray (y_list)

#Pick real samples from the dataset 
def generate_real_samples (dataset, n_samples): 
  images, labels= dataset 
  ix= randint (0, images.shape[0], n_samples) 
  X, labels= images[ix], labels[ix] 
  y= ones ((n_samples,1)) 
  return [X, labels], y

# Generate the latent points for generator 
def generate_latent_points (latent_dim, n_samples): 
  z_input= randn (latent_dim * n_samples) # creates an array with random values
  z_input= z_input.reshape (n_samples, latent_dim) 
  return z_input

def generate_fake_samples (generator, latent_dim, n_samples): 
  z_input= generate_latent_points (latent_dim, n_samples) 
  fake_images= generator.predict (z_input) 
  y= zeros ((n_samples,1))
  return fake_images,y

def train(gen_model, disc_unsup, disc_sup, gan_model, dataset, latent_dim, n_epochs=20, n_batch=100):

	
    # select supervised dataset for training.
    #Remember that we are not using all 60k images, just a subset (100 images, 10 per class. )
	X_sup, y_sup = select_supervised_sample(dataset)
	#print(X_sup.shape, y_sup.shape)
	
	bat_per_epo = int(dataset[0].shape[0] / n_batch)
	# iterations
	n_steps = bat_per_epo * n_epochs
	
	half_batch = int(n_batch / 2)
	print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, 
                                                              n_batch, half_batch, 
                                                              bat_per_epo, n_steps))
	
    #  enumerate epochs
	for i in range(n_steps):
		# update supervised discriminator (disc_sup) on real samples.
        #Remember that we use real labels to train as this is supervised. 
        #This is the discriminator we really care about at the end.
        #Also, this is a multiclass classifier, not binary. Therefore, our y values 
        #will be the real class labels for MNIST. (NOT 1 or 0 indicating real or fake.)
		[Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)
		sup_loss, sup_acc = disc_sup.train_on_batch(Xsup_real, ysup_real)
        
		# update unsupervised discriminator (disc_unsup) - just like in our regular GAN.
        #Remember that we will not train on labels as this is unsupervised, just binary as in our regular GAN.
        #The y_real below indicates 1s telling the discriminator that these images are real. 
        #do not confuse this with class labels. 
        #We will discard this discriminator at the end. 
		[X_real, _], y_real = generate_real_samples(dataset, half_batch) #
		d_loss_real = disc_unsup.train_on_batch(X_real, y_real)
        #Now train on fake. 
		X_fake, y_fake = generate_fake_samples(gen_model, latent_dim, half_batch)
		d_loss_fake = disc_unsup.train_on_batch(X_fake, y_fake)
        
		# update generator (gen) - like we do in regular GAN.
        #We can discard this model at the end as our primary goal is to train a multiclass classifier (sup. disc.)
		X_gan, y_gan = generate_latent_points(latent_dim, n_batch), ones((n_batch, 1))
		gan_loss = gan_model.train_on_batch(X_gan, y_gan)
        
		# summarize loss on this batch
		print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (i+1, sup_loss, sup_acc*100, d_loss_real, d_loss_fake, gan_loss))

latent_dim= 100 
disc= define_discriminator () 
disc_supervised= supervised_descriminator (disc) 
disc_unsupervised= unsupervised_discriminator (disc) 
gen_model= define_generator (latent_dim) 
gan_model= define_gan (gen_model, disc_unsupervised) 
dataset= load_real_samples ()

train (gen_model, disc_unsupervised, disc_supervised,gan_model, dataset, latent_dim, n_epochs=50, n_batch=100 )

filename= 'supervised_discriminator.h5'
disc_supervised.save (filename)

# Testing 
(_,_), (test_x, test_y)= load_data ()

test_x= expand_dims (test_x, axis=-1)

test_x.shape

test_x= test_x.astype ('float32')

test_x= (test_x-127.5)/127.5

_, test_acc= disc_supervised.evaluate (test_x, test_y, verbose=0)

test_acc

